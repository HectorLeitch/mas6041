{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letters\n",
    "\n",
    "Run the source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"code\\letters.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'chi', 'chic', 'chichi', 'chili', 'chill', 'chilli', 'chilly', 'chit', 'chou', 'ci', 'city', 'cl', 'clio', 'clot', 'cloth', 'clotho', 'clout', 'cloy', 'clutch', 'co', 'cocci', 'coccyx', 'coil', 'col', 'colic', 'colo', 'colt', 'coo', 'cool', 'coolly', 'coot', 'cot', 'couch', 'cox', 'coy', 'coyly', 'cthulhu', 'cu', 'cull', 'cult', 'cut', 'cyclic', 'h', 'hi', 'hill', 'hilly', 'hilt', 'hit', 'hitch', 'ho', 'hohhot', 'holly', 'holt', 'holy', 'hooch', 'hoot', 'hootch', 'hot', 'hotly', 'huh', 'hui', 'hull', 'hut', 'hutch', 'hutu', 'i', 'icily', 'icy', 'ill', 'illicit', 'illicitly', 'io', 'it', 'itch', 'itchy', 'ito', 'j', 'jill', 'jilt', 'jo', 'jollity', 'jolly', 'jolt', 'jot', 'joy', 'juicy', 'jul', 'julio', 'july', 'jut', 'l', 'li', 'licit', 'lilith', 'lilly', 'lilt', 'lily', 'lit', 'litchi', 'lo', 'loci', 'loco', 'loll', 'loot', 'lot', 'loth', 'lott', 'lotto', 'lou', 'lout', 'lox', 'lt', 'lu', 'lucio', 'lucy', 'lull', 'lully', 'lulu', 'lyly', 'lyx', 'o', 'occult', 'oct', 'oh', 'ohio', 'oho', 'oil', 'oilcloth', 'oily', 'otto', 'ouch', 'out', 'ox', 't', 'tc', 'th', 'tho', 'thoth', 'thou', 'thy', 'ti', 'tic', 'till', 'tillich', 'tilt', 'tit', 'tito', 'tl', 'to', 'toil', 'tojo', 'toll', 'too', 'tool', 'toot', 'tooth', 'toothy', 'tot', 'toto', 'touch', 'touchy', 'tout', 'toxic', 'toxicity', 'toy', 'tull', 'tut', 'tutu', 'tux', 'ty', 'tycho', 'u', 'uh', 'utility', 'x', 'y', 'yo', 'you', 'youth'] [4, 202, 5526, 269064, 22829, 96034, 257085, 418136, 10850, 12665, 15, 3645, 70, 6846, 11323, 113810, 919065, 129782, 3337, 1197728, 59, 20508, 1645659, 8166, 785, 60680, 7440, 11433, 664, 8650, 418598, 11312, 1027, 108959, 1269, 301, 37569, 17172170, 103, 8815, 11477, 1071, 667341, 7, 18, 8730, 38012, 11392, 986, 108797, 62, 1371839, 38056, 11436, 3450, 108478, 11315, 1197236, 1030, 38298, 953, 227, 8818, 1074, 108885, 13053, 1, 37434, 287, 793, 14394227, 560035015, 56, 89, 9890, 39172, 694, 3, 8726, 11388, 58, 4854941, 38052, 11432, 1026, 300, 34829, 828, 75364, 3490, 1070, 6, 17, 118960, 1246559, 38011, 11391, 3405, 985, 269847, 61, 1876, 7200, 8773, 11314, 1029, 10346, 11677, 84882, 1150, 11798, 1271, 94, 105, 75125, 3251, 8817, 38099, 12810, 3416, 1238, 5, 1388766, 1017, 82, 6858, 687, 742, 151481852, 3404, 7716, 9905, 1072, 115, 8, 52, 85, 690, 113825, 12669, 327, 19, 503, 8731, 13068503, 11393, 987, 7642, 74, 63, 8170, 7081, 8775, 668, 8654, 11316, 113803, 435905, 1031, 7686, 108963, 431065, 11800, 61168, 53369049, 305, 8819, 1075, 13054, 1317, 30, 83036, 9, 86, 4854375, 10, 2, 57, 1146, 114281]\n"
     ]
    }
   ],
   "source": [
    "words_3x3, numbers_3x3 = make_2vocab(file='words_3x3.txt')\n",
    "print(words_3x3, numbers_3x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.1\n",
    "\n",
    "Here we demonstrate the kernels to detect a letter from its representation in a 3x3 grid. Modify `letter` to any of the given letters and check the output is the one-hot encoding of the letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G = \n",
      "[[1 1 1]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "output = [[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# The letters in order are i,y,j,c,o,l,h,t,u,x\n",
    "letter = 'c'\n",
    "\n",
    "G = letter_pixels[letter]\n",
    "print(f\"G = \\n{G}\")\n",
    "G = tf.convert_to_tensor(G.reshape(1,3,3))\n",
    "M = make_explicit_model()\n",
    "print(f\"output = {M.predict(G, verbose=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that it rejects non-letters by setting the output to all zeros. A random choice of `G` will give us a non-letter 502 times out of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G = \n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "output = [[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "G = np.random.choice(a=[0, 1], size=(3, 3), p=[0.5, 0.5])\n",
    "print(f\"G = \\n{G}\")\n",
    "G = tf.convert_to_tensor(G.reshape(1,3,3))\n",
    "M = make_explicit_model()\n",
    "print(f\"output = {M.predict(G, verbose=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the function below tests whether the model output classifies all possible 512 3x3 images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_explicit_model(M=M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.2\n",
    "\n",
    "We now demonstrate the convolutional model using the kernels from the model above to search for letters in a grid of height $h$ and width $w$. The variable `word` can be set to any word containing our specified letters, and the function `make_ragged_grid()` will choose appropriate values for $h$ and $w$ and create a grid containing the word.\n",
    "\n",
    "The output of the model contains a series of one-hot vectors showing the detected letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACoCAYAAAB9n+1TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADOklEQVR4nO3cMW7jQBBFQc2C979yO93IgW2yQbyqA0hfEkk8TKAzM/MBALL+bQ8AAHaJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4q7tAUDDOWd7wiv4Hzg2OBkAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABA3LU9AGiYmdvf45xz6+s/8RmecPf39Pn4vd/GyQAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADiru0B/zvnbE94hZnZnvAad19Tfgve6Inr9onnufvv7zgZAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAg7toe8LSZ2Z4A3MT9DT/jZAAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABx1/aAp51ztif82sxsTwD4lufUuzgZAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQNyZmdkeAQDscTIAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADivgC4BDBC9KiCiQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "word = 'city'\n",
    "\n",
    "G = make_ragged_grid(word)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-G, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "G = tf.convert_to_tensor(G.reshape(1, G.shape[0], G.shape[1]))\n",
    "GM = make_conv_model(G.shape[1], G.shape[2])\n",
    "print(GM.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.3\n",
    "\n",
    "This example demonstrates the model above with the additional of a layer to count up the occurences of each letter. The output is a single vector containing the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACoCAYAAAB9n+1TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADQElEQVR4nO3cQU7EQAwAQQbl/182Fw6cEBJszG5XfSCOJhq1fMiZmXkDALLetwcAAHaJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4q7tAeg65zz8Gf6p9TN3nMUr8D3xqmwGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAEHdtDwDsm5mHP+Oc8/Bn3PEer+BVzuLR71H6nmwGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAEHdtD/DVOWd7hKcwM9sjAE/sjjvkjvvcXfh3bAYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIi7tge428xsj8AnZ9HivOH/shkAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABA3LU9wN3OOdsj/NrMbI8A8C331HOxGQCAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAEDctT3AVzOzPQIA5NgMAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAuA9DOjJD+JypfwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   [0.]]]]\n"
     ]
    }
   ],
   "source": [
    "word = 'city'\n",
    "\n",
    "G = make_ragged_grid(word)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-G, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "G = tf.convert_to_tensor(G.reshape(1, G.shape[0], G.shape[1]))\n",
    "SM = make_sum_model(G.shape[1], G.shape[2])\n",
    "print(SM.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.4\n",
    "\n",
    "The following code demonstrates the model that recognises words. The output will be a series of integers from 1 to 10 representing each letter. The sequence is sorted so that the detected letters are moved to the front of the sequence, and the zeros are moved to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACoCAYAAAB9n+1TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADLklEQVR4nO3cMW6EQBAAQY91///yOHXIycJr01UxgiFY1JqA2d39AACyPk8PAACcJQYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIh7nR6Arpm5/Rn+qQVnPOF8P+EdrrIZAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQNzr9AAAPM/u3v6Mmbn1/r/xDn+FzQAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADiXlcvnJk753iM3T09ArzN+b7G+eapbAYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIib3d1LF87cPcvHxVGAf8g3BP4umwEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEvU4P8N3MnB7hx3b39AgA8BabAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMTN7u7pIQCAc2wGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABA3BfTkCtHQfhhMQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 1. 8. 2. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "word = 'city'\n",
    "\n",
    "G = make_ragged_grid(word)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-G, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "G = tf.convert_to_tensor(G.reshape(1, G.shape[0], G.shape[1]))\n",
    "GRUM = make_gru_model(G.shape[1], G.shape[2])\n",
    "print(GRUM.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the output at various stages to verify the model is working as described. The code below shows the output of layer $\\mathcal{L}^{\\left[3\\right]}$, which is the sequence $\\left\\{x_t\\right\\}_{t=1}^{w_g}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000242A46C32E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[[4.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [8.]\n",
      "  [0.]\n",
      "  [2.]]]\n"
     ]
    }
   ],
   "source": [
    "GRUM_integers = tf.keras.Model(inputs=GRUM.input, outputs=GRUM.layers[5].output)\n",
    "print(GRUM_integers.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to produce the sequence of indicators in layer $\\mathcal{L}^{\\left[5\\right]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000242A46C3B50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]]\n"
     ]
    }
   ],
   "source": [
    "GRUM_indicator = tf.keras.Model(inputs=GRUM.input, outputs=GRUM.layers[7].output)\n",
    "print(GRUM_indicator.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to produce the series of positional indicators $\\left\\{\\pmb{p}_t\\right\\}_{t=1}^{w_g}$ in layer $\\mathcal{L}^{\\left[8\\right]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "GRUM_position = tf.keras.Model(inputs=GRUM.input, outputs=GRUM.layers[10].output)\n",
    "print(GRUM_position.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full combination passed to the final GRU layer is then as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [8. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [2. 0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "GRUM_final_input = tf.keras.Model(inputs=GRUM.input, outputs=GRUM.layers[13].output)\n",
    "print(GRUM_final_input.predict(G, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.5\n",
    "\n",
    "The alternative model below shows how the sequence $\\left\\{\\pmb{p}_t\\right\\}_{t=1}^{w_g}$ could also be created in a more intuitive way using a GRU layer with an initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACoCAYAAAB9n+1TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADO0lEQVR4nO3cwY2DQBQFQc+K/FP+Pq3kAAxj6KoA0LNGoNYcvGZmXgBA1t/uAQDAXmIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIC4Y/cAYL+11u4Jt+A/2ngqNwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIO3YPgDOttU59/syc+vyrXPE7zj6L1+s553G2p5yF9/t73AwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAg7tg94NNaa/eEW5iZ3ROAG7viG3LF99y38HvcDABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAEHfsHnC1mdk9AZK8e/C73AwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgbs3M7B7xb621e8It/NCRAfAAbgYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQd+we8Glmdk8AgBw3AwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIO4N2KgwQr4y2AYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to GRU layer:\n",
      " [[[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]]\n",
      "Output of GRU layer:\n",
      " [[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]]]\n",
      "Final output:\n",
      " [[4. 1. 8. 2. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "word = 'city'\n",
    "\n",
    "G = make_ragged_grid(word)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-G, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "G = tf.convert_to_tensor(G.reshape(1, G.shape[0], G.shape[1]))\n",
    "GRUaM = make_alt_gru_model(G.shape[1], G.shape[2])\n",
    "\n",
    "GRUaM_indicator = tf.keras.Model(inputs=GRUaM.input, outputs=GRUaM.layers[7].output)\n",
    "print(f\"Input to GRU layer:\\n {GRUaM_indicator.predict(G, verbose=0)}\")\n",
    "\n",
    "GRUaM_position = tf.keras.Model(inputs=GRUaM.input, outputs=GRUaM.layers[8].output)\n",
    "print(f\"Output of GRU layer:\\n {GRUaM_position.predict(G, verbose=0)}\")\n",
    "\n",
    "print(f\"Final output:\\n {GRUaM.predict(G, verbose=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5.6\n",
    "\n",
    "A full model with lookup layers added can be tested below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAACoCAYAAAB9n+1TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADOUlEQVR4nO3csWrEMBQAwVPw///yS3tFCkOiU/DO1MY8Y0ssKrxmZl4AQNbX6QEAgLPEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcdfpAWCntdbW+/tnF/xs99p7vfavvyc8w11OBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABB33b1wrbVzjseYmdMjwL9kD7nnKXvIJ55j9zf1lHdxh5MBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxF2nB3g3M6dHADb5xPpea229vz2Kp3IyAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABA3HV6gHdrrdMj/NrMnB6BN94HPJf1/XecDABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBuzcycHgIAOMfJAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABD3DZ5wMEBvVMtEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n"
     ]
    }
   ],
   "source": [
    "word = 'city'\n",
    "\n",
    "G = make_ragged_grid(word)\n",
    "plt.axis('off')\n",
    "plt.imshow(1-G, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "G = tf.convert_to_tensor(G.reshape(1, G.shape[0], G.shape[1]))\n",
    "FM = make_full_model(G.shape[1], G.shape[2])\n",
    "# decode() just takes the raw string from the tensor object\n",
    "[print(z.decode()) for z in FM.predict(G, verbose=0).reshape(1,)]\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
